{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import re\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "pd.options.display.max_columns = 160\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Functions \n",
    "\n",
    "# Function to clean strings and convert to lowercase\n",
    "def transform_place_name(place_name):\n",
    "    # if place_name is str: \n",
    "    place_name = place_name.lower()\n",
    "    place_name = place_name.strip()\n",
    "    place_name = place_name.replace('-', '')\n",
    "        \n",
    "        # Split and swap the parts if there's a comma\n",
    "    if ',' in place_name:\n",
    "        parts = place_name.split(',')\n",
    "        if len(parts) > 1:\n",
    "            # Strip extra spaces, rearrange parts, and replace spaces with hyphens\n",
    "            new_place_name = (parts[1].strip() + ' ' + parts[0].strip()).replace(' ', '')\n",
    "            return new_place_name\n",
    "\n",
    "    # Replace spaces with hyphens for names without a comma\n",
    "    place_name = re.sub(r\"[^\\w\\s]\", '', place_name)\n",
    "    place_name = place_name.replace(' ', '')\n",
    "    return place_name.replace('-', '')\n",
    "\n",
    "# Define a function to adjust the year in the 'Datum' column\n",
    "def adjust_year(date):\n",
    "    try:\n",
    "        return date.replace(year=date.year - 1)\n",
    "    except ValueError:\n",
    "        if date.month == 2 and date.day == 29:\n",
    "            return pd.NaT  # Use pandas Not a Time to flag this date for removal\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the folder path\n",
    "folder_path = 'data_traffic_jams'\n",
    "\n",
    "# Get the list of files in the folder\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "dfs = []\n",
    "\n",
    "# Define the search pattern once, escaping parentheses\n",
    "search_pattern = 'Spitsfile \\(geen oorzaak gemeld\\)|File buiten spits \\(geen oorzaak gemeld\\)'\n",
    "\n",
    "# Define valid years set for checking file year\n",
    "valid_years = {'2019', '2020', '2021', '2022', '2023', '2024'}\n",
    "\n",
    "# Iterate over the files\n",
    "for file in files:\n",
    "    year = file[:4]\n",
    "    # Check if the file is from 2018 onwards\n",
    "    if year in valid_years:\n",
    "        # Read each file as a dataframe\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path, delimiter=';')\n",
    "        \n",
    "        # Filter rows where 'Oorzaak_1' contains any of the specified values\n",
    "        filtered_df = df[df['OorzaakGronddetail'].str.contains(search_pattern, case=False, regex=True)]\n",
    "        \n",
    "        # Append the filtered dataframe to the list\n",
    "        dfs.append(filtered_df)\n",
    "\n",
    "# Concatenate the dataframes into one big dataframe\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Efficiently convert and combine date and time columns\n",
    "df['DatumTijdFileBegin'] = pd.to_datetime(df['DatumFileBegin'] + ' ' + df['TijdFileBegin'])\n",
    "df['DatumTijdFileEind'] = pd.to_datetime(df['DatumFileEind'] + ' ' + df['TijdFileEind'])\n",
    "df['DatumFileBeginInt'] = df['DatumTijdFileBegin'].dt.strftime('%Y%m%d').astype(int)\n",
    "df['DatumFileEindInt'] = df['DatumTijdFileEind'].dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "\n",
    "# Perform string replacements in bulk instead of looping through each column\n",
    "for suffix in ['-Noord', '-Oost', '-Zuid', '-West']:\n",
    "    df['KopWegvakNaar'] = df['KopWegvakNaar'].str.replace(suffix, '', regex=False)\n",
    "    df['KopWegvakVan'] = df['KopWegvakVan'].str.replace(suffix, '', regex=False)\n",
    "    df['TrajVan'] = df['TrajVan'].str.replace(suffix, '', regex=False)\n",
    "    df['TrajNaar'] = df['TrajNaar'].str.replace(suffix, '', regex=False)\n",
    "\n",
    "\n",
    "# Fill NaN values and convert columns to strings efficiently\n",
    "columns_to_convert = ['KopWegvakVan', 'KopWegvakNaar']\n",
    "df[columns_to_convert] = df[columns_to_convert].fillna('').astype(str)\n",
    "\n",
    "# Apply transformation to place names\n",
    "df['KopWegvakVan'] = df['KopWegvakVan'].apply(transform_place_name)\n",
    "df['KopWegvakNaar'] = df['KopWegvakNaar'].apply(transform_place_name)\n",
    "df['TrajNaar'] = df['TrajNaar'].apply(transform_place_name)\n",
    "df['TrajVan'] = df['TrajVan'].apply(transform_place_name)\n",
    "\n",
    "# Correctly handle decimal separators and convert to float\n",
    "for column in ['FileZwaarte', 'GemLengte', 'FileDuur']:\n",
    "    df[column] = df[column].str.replace(',', '.').astype(float)\n",
    "\n",
    "# Define a dictionary with replacements\n",
    "replacements = {\n",
    "    'mariënheem': 'marienheem',\n",
    "    'belgië': 'breda',\n",
    "    'mariănheem': 'marienheem',\n",
    "    'portzélande': 'ouddorp',\n",
    "    'schöninghsdorf': 'arnhem',\n",
    "    'belgiă': 'breda',\n",
    "    'portzălande': 'ouddorp',\n",
    "    'portzalande': 'ouddorp',\n",
    "    'MariĂ«nheem': 'zwolle', \n",
    "    'nederland': 'roosendaal'  # Assuming 'MariĂ«nheem' is equivalent to 'zwolle'\n",
    "}\n",
    "\n",
    "# Apply the replacements to both 'KopWegvakNaar' and 'KopWegvakVan' columns\n",
    "df['KopWegvakNaar'] = df['KopWegvakNaar'].replace(replacements, regex=True)\n",
    "df['KopWegvakVan'] = df['KopWegvakVan'].replace(replacements, regex=True)\n",
    "\n",
    "# Convert hecotmeter columns to float\n",
    "df['HectometerKop'] = df['HectometerKop'].str.replace(',', '.').astype(float)\n",
    "df['HectometerStaart'] = df['HectometerStaart'].str.replace(',', '.').astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuel data\n",
    "\n",
    "# Load and preprocess the fuel data\n",
    "fuel_df = pd.read_csv('/Users/floris/Desktop/DSS/Thesis/data_fuel/Observations.csv', delimiter=';')\n",
    "\n",
    "fuel_df.drop([\"StringValue\", \"ValueAttribute\"], axis=1, inplace=True)\n",
    "\n",
    "# Mapping measure codes to fuel types\n",
    "measure_to_fuel = {\n",
    "    'A047220': 'Gasoline',\n",
    "    'A047219': 'Diesel',\n",
    "    'A047221': 'LPG'\n",
    "}\n",
    "fuel_df['Measure'] = fuel_df['Measure'].map(measure_to_fuel).fillna(fuel_df['Measure'])\n",
    "\n",
    "# Pivot the DataFrame so each fuel type is a column with the date as rows\n",
    "fuel_df = fuel_df.pivot_table(index='Perioden', columns='Measure', values='Value', aggfunc='first')\n",
    "\n",
    "# Merge the data\n",
    "df = df.merge(fuel_df, left_on='DatumFileBeginInt', right_index=True, how='left')\n",
    "\n",
    "df['Gasoline'] = df['Gasoline'].str.replace(',', '.').astype(float)\n",
    "df['Diesel'] = df['Diesel'].str.replace(',', '.').astype(float)\n",
    "df['LPG'] = df['LPG'].str.replace(',', '.').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the newplacenames data\n",
    "newplacenames_df = pd.read_csv('data_area/nieuw_placenames.csv', sep=';', encoding='latin1')\n",
    "newplacenames_df['Nieuwe'] = newplacenames_df['Nieuwe'].apply(transform_place_name)\n",
    "newplacenames_df['UniqueKopWegvakNaar'] = newplacenames_df['UniqueKopWegvakNaar'].apply(transform_place_name)\n",
    "replacement_dict = newplacenames_df.set_index('UniqueKopWegvakNaar')['Nieuwe'].to_dict()\n",
    "\n",
    "# Create two new columns for KopWegvakVan and KopWegvakNaar with their new values\n",
    "df['KopWegvakVan_New'] = df['KopWegvakVan'].map(replacement_dict)\n",
    "df['KopWegvakNaar_New'] = df['KopWegvakNaar'].map(replacement_dict)\n",
    "\n",
    "df['KopWegvakVan_New'] = df['KopWegvakVan_New'].astype('string')\n",
    "df['KopWegvakNaar_New'] = df['KopWegvakNaar_New'].astype('string')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the '4pp.csv' dataset\n",
    "area_df = pd.read_csv('/Users/floris/Desktop/DSS/Thesis/data_area/4pp.csv', delimiter=',')\n",
    "\n",
    "# Convert 'latitude' and 'longitude' to numeric, coercing non-numeric values to NaN\n",
    "area_df['latitude'] = pd.to_numeric(area_df['latitude'], errors='coerce')\n",
    "area_df['longitude'] = pd.to_numeric(area_df['longitude'], errors='coerce')\n",
    "\n",
    "# Calculate average longitude and latitude for each distinct place name, ignoring NaNs\n",
    "area_df = area_df.groupby('woonplaats').agg({'latitude': 'mean', 'longitude': 'mean', 'provincie': 'first'}).reset_index()\n",
    "\n",
    "# Apply transformations to the 'PlaceName' column\n",
    "area_df['woonplaats'] = area_df['woonplaats'].astype('string')\n",
    "area_df['woonplaats'] = area_df['woonplaats'].apply(transform_place_name)\n",
    "\n",
    "# Merge the 'provincie' column from 'area_df' with the main DataFrame 'df' based on 'KopWegvakVan_New' and 'KopWegvakNaar_New'\n",
    "df = pd.merge(df, area_df, left_on='KopWegvakVan_New', right_on='woonplaats', how='left')\n",
    "df = pd.merge(df, area_df, left_on='KopWegvakNaar_New', right_on='woonplaats', how='left')\n",
    "\n",
    "# Drop the additional 'PlaceName' columns\n",
    "# df.drop(['PlaceName_x', 'PlaceName_y', 'provincie_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df= pd.read_csv('data_weather/stations.csv')\n",
    "\n",
    "# Remove NaN values from critical columns to avoid errors\n",
    "df = df.dropna(subset=['latitude_x', 'longitude_x'])\n",
    "station_df = station_df.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# Extract unique coordinates from merged_df\n",
    "unique_coords = df[['latitude_x', 'longitude_x']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Create a k-d tree with station coordinates\n",
    "tree = cKDTree(station_df[['latitude', 'longitude']].values)\n",
    "\n",
    "# Query the tree for the two nearest neighbors of each unique coordinate\n",
    "distances, indices = tree.query(unique_coords.values, k=2)  # k=2 for the two nearest stations\n",
    "\n",
    "# Extract the closest station coordinates using the indices from the k-d tree query\n",
    "closest_stations = station_df.iloc[indices[:, 0]].reset_index().rename(columns={'index': 'original_index', 'station_code': 'closest_station_code'})\n",
    "second_closest_stations = station_df.iloc[indices[:, 1]].reset_index().rename(columns={'index': 'original_index', 'station_code': 'second_closest_station_code'})\n",
    "\n",
    "# Include the station codes from the closest and second closest stations to map back\n",
    "unique_coords['closest_station_code'] = closest_stations['closest_station_code']\n",
    "unique_coords['second_closest_station_code'] = second_closest_stations['second_closest_station_code']\n",
    "\n",
    "# Merge this information back onto the original merged_df to map each entry to its closest and second closest station\n",
    "# Ensure latitude_x and longitude_x uniquely identify rows in merged_df,\n",
    "# if not, additional merging criteria may be needed.\n",
    "merged_df = pd.merge(df, unique_coords, on=['latitude_x', 'longitude_x'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6d/z_ycnlvn2kb2n3lr05wsg2cm0000gn/T/ipykernel_5695/1821816312.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  weather_df = pd.concat(dfs, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "folder_path = '/Users/floris/Desktop/DSS/Thesis/data_weather'\n",
    "\n",
    "# Get the list of file names in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "dfs = []\n",
    "\n",
    "# Iterate over the file names\n",
    "for file_name in file_names:\n",
    "    # Check if the file is a ZIP file\n",
    "    if file_name.endswith('.zip'):\n",
    "        # Construct the file path\n",
    "        zip_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Open the ZIP file\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            # List all the contained files to ensure we only process JSON files\n",
    "            contained_files = zip_ref.namelist()\n",
    "            for contained_file in contained_files:\n",
    "                if contained_file.endswith('.json'):\n",
    "                    # Extract JSON file content\n",
    "                    with zip_ref.open(contained_file) as file:\n",
    "                        data = json.load(file)\n",
    "                    \n",
    "                    # Convert the JSON data to a dataframe\n",
    "                    df = pd.DataFrame(data)\n",
    "                    \n",
    "                    # Append the dataframe to the list\n",
    "                    dfs.append(df)\n",
    "\n",
    "# Concatenate the dataframes\n",
    "if dfs:\n",
    "    weather_df = pd.concat(dfs, ignore_index=True)\n",
    "else:\n",
    "    print(\"No JSON files found in the ZIP archives.\")\n",
    "\n",
    "weather_df = weather_df.drop_duplicates()\n",
    "weather_df['hour'] -=1 \n",
    "\n",
    "# weather_df.to_csv('weather_data.csv', index=False)\n",
    "# weather_df = pd.read_csv('weather_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.copy()\n",
    "# Change the NaN values in the Boolean columns to 0 \n",
    "columns_to_fill = ['M', 'R', 'S', 'O', 'Y']\n",
    "weather_df[columns_to_fill] = weather_df[columns_to_fill].fillna(0)\n",
    "\n",
    "# Example: Ensuring 'station_index' and 'station_code' are both integers\n",
    "df['closest_station_code'] = df['closest_station_code'].astype(int)\n",
    "weather_df['station_code'] = weather_df['station_code'].astype(int)\n",
    "\n",
    "# Similarly, make sure that 'HourOfDay' and 'hour' are of the same type, as well as 'DatumFileBegin' and 'date'\n",
    "df['DatumTijdFileBegin'] = pd.to_datetime(df['DatumTijdFileBegin'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df['HourOfDay'] = df['DatumTijdFileBegin'].dt.hour \n",
    "df['HourOfDay'] = df['HourOfDay'].astype(int)\n",
    "weather_df['hour'] = weather_df['hour'].astype(int)\n",
    "\n",
    "# Similarly, make sure that 'HourOfDay' and 'hour' are of the same type, as well as 'DatumFileBegin' and 'date'\n",
    "df['DatumFileBeginInt'] = df['DatumFileBeginInt'].astype(int)\n",
    "weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "weather_df['date'] = weather_df['date'].dt.strftime('%Y%m%d')\n",
    "weather_df['date'] = weather_df['date'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First merge with the closest station\n",
    "df = pd.merge(\n",
    "    df, weather_df,\n",
    "    how='left',\n",
    "    left_on=['HourOfDay', 'DatumFileBeginInt', 'closest_station_code'],\n",
    "    right_on=['hour', 'date', 'station_code'],\n",
    "    suffixes=('', '_closest')\n",
    ")\n",
    "\n",
    "# Second merge with the second closest station\n",
    "result_second_df = pd.merge(\n",
    "    df, weather_df,\n",
    "    how='left',\n",
    "    left_on=['HourOfDay', 'DatumFileBeginInt', 'second_closest_station_code'],\n",
    "    right_on=['hour', 'date', 'station_code'],\n",
    "    suffixes=('', '_second_closest')\n",
    ")\n",
    "\n",
    "columns_to_check =['DD', 'FH', 'FF', 'FX', 'T', 'SQ', 'Q', 'DR', 'RH', 'P', 'VV', 'N', 'U']\n",
    "\n",
    "# Replace NaN values from the first merge with values from the second merge\n",
    "for column in columns_to_check:\n",
    "    df[column] = df[column].fillna(result_second_df[column + '_second_closest'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data_public_transport'  # Specify your directory path\n",
    "files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop through each file and append its DataFrame to the list\n",
    "for file in files:\n",
    "    file_path = os.path.join(directory, file)  # Create the full path to the file\n",
    "    dfs = pd.read_csv(file_path)  # Read the CSV file\n",
    "    df_list.append(dfs)  # Append the DataFrame to the list\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "pt_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Ensure the Datum column exists\n",
    "if 'Datum' not in pt_df.columns or pt_df['Datum'].isnull().all():\n",
    "    pt_df['Datum'] = pd.NaT\n",
    "\n",
    "# Filter to get indices where Datum is NaN\n",
    "na_indices = pt_df[pt_df['Datum'].isna()].index\n",
    "\n",
    "# Step 2: Construct Datum only for NaN entries\n",
    "for idx in na_indices:\n",
    "    # Create a date string from 'Dag', 'Maand', 'Jaar'\n",
    "    day = pt_df.loc[idx, 'Dag']\n",
    "    month = pt_df.loc[idx, 'Maand']\n",
    "    year = pt_df.loc[idx, 'Jaar']\n",
    "    if pd.notna(day) and pd.notna(month) and pd.notna(year):\n",
    "        date_str = f\"{int(day)}-{int(month)}-{int(year)}\"\n",
    "        # Convert the string to a datetime object\n",
    "        pt_df.loc[idx, 'Datum'] = pd.to_datetime(date_str, format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "# Drop 'Jaar', 'Maand', and 'Dag' columns if no longer needed\n",
    "pt_df.drop(['Jaar', 'Maand', 'Dag'], axis=1, inplace=True)\n",
    "\n",
    "# Convert 'Datum' to datetime\n",
    "pt_df['Datum'] = pd.to_datetime(pt_df['Datum'], format='%d-%m-%Y', errors='coerce')\n",
    "pt_df['Uur'] = pd.to_numeric(pt_df['Uur'], errors='coerce')\n",
    "\n",
    "# Filter rows for the year 2020 and create a new DataFrame to avoid SettingWithCopyWarning\n",
    "data_2020 = pt_df[pt_df['Datum'].dt.year == 2020].copy()\n",
    "\n",
    "# Apply the function using loc to avoid SettingWithCopyWarning\n",
    "data_2020['Datum'] = data_2020['Datum'].apply(adjust_year)\n",
    "data_2020.dropna(subset=['Datum'], inplace=True)\n",
    "\n",
    "# Handle references and renaming within data_2020 to avoid warnings\n",
    "data_2020['Aantal_check_ins'] = data_2020['Referentie_vorig_jaar']\n",
    "\n",
    "data_2020 = data_2020.drop(['Referentie_pre_COVID_19', 'Referentie_vorig_jaar', \n",
    "                            'Delta_actueel_pre_COVID_19', 'Delta_actueel_vorig_jaar'], axis=1)\n",
    "\n",
    "# Drop the same columns from pt_df\n",
    "pt_df = pt_df.drop(['Referentie_pre_COVID_19', 'Referentie_vorig_jaar', 'Delta_actueel_pre_COVID_19', 'Delta_actueel_vorig_jaar'], axis=1)\n",
    "\n",
    "# Concatenate data_2020 back to pt_df\n",
    "pt_df = pd.concat([pt_df, data_2020], ignore_index=True)\n",
    "pt_df = pt_df.sort_values(by=['Datum', 'Uur'])\n",
    "pt_df = pt_df.drop_duplicates()\n",
    "data_2020.dropna(subset=['Datum'], inplace=True)\n",
    "\n",
    "# Convert 'Datum' column to integer format YYYYMMDD\n",
    "pt_df['Datum'] = pt_df['Datum'].dt.strftime('%Y%m%d').astype(int)\n",
    "\n",
    "pt_df['Aantal_check_ins'] = np.where(pt_df['Datum'] < 20230000, \n",
    "                                     pt_df['Aantal_check_ins'] * 1000, \n",
    "                                     pt_df['Aantal_check_ins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column for the previous hour, subtracting 1 only if HourOfDay is not 0\n",
    "df['PrevHourOfDay'] = np.where(df['HourOfDay'] != 0, df['HourOfDay'] - 1, df['HourOfDay'])\n",
    "\n",
    "# Now perform the merge\n",
    "df = pd.merge(df, pt_df,\n",
    "                     how='left',\n",
    "                     left_on=['HourOfDay', 'DatumFileBeginInt'],\n",
    "                     right_on=['Uur', 'Datum'],\n",
    "                     suffixes=('', '_current'))\n",
    "\n",
    "# Merge again for the previous hour\n",
    "df = pd.merge(df, pt_df,\n",
    "                     how='left',\n",
    "                     left_on=['PrevHourOfDay', 'DatumFileBeginInt'],\n",
    "                     right_on=['Uur', 'Datum'],\n",
    "                     suffixes=('_current', '_previous'))\n",
    "\n",
    "# Drop the intermediate column\n",
    "df.drop('PrevHourOfDay', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "free_day_df = pd.read_csv('data_freedays/freedays20192024.csv')\n",
    "\n",
    "# Convert DatumFileBeginInt column to datetime\n",
    "df['DatumFileBeginInt'] = pd.to_datetime(df['DatumFileBeginInt'], format='%Y%m%d')\n",
    "\n",
    "# Convert Date column to datetime\n",
    "free_day_df['Date'] = pd.to_datetime(free_day_df['Date'])\n",
    "\n",
    "# Set index of free_day_df to 'Date' column\n",
    "free_day_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Create a new column called 'freeday' based on comparison of dates\n",
    "df['freeday'] = df['DatumFileBeginInt'].isin(free_day_df.index).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('full_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
