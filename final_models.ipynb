{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_for_machine_learning.csv')\n",
    "\n",
    "# Convert the date columns to datetimeint\n",
    "df['DatumFileBeginInt'] = df['DatumFileBeginInt'].str.replace('-', '').astype(int)\n",
    "\n",
    "# Define the road section researched\n",
    "df = df[df['RouteOms']== 'A4']\n",
    "\n",
    "#  Remove duplicate columns and unnecessary columns\n",
    "columns_to_remove = ['RouteOms', 'latitude_x', 'longitude_x', 'provincie_x', 'latitude_y', \n",
    "                     'longitude_y', 'RouteLet', 'GemLengte', 'FileDuur', 'NLSitNummer',\n",
    "                    'RouteNum', 'RouteLet_encoded', 'HectometerStaart', 'DatumFileBeginInt', \n",
    "                    'station_code', 'DatumTijdFileEind'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_remove)\n",
    "\n",
    "df['DatumTijdFileBegin'] = pd.to_datetime(df['DatumTijdFileBegin']) \n",
    "\n",
    "# Define target column to drop\n",
    "columns_to_drop = ['FileZwaarte', 'DatumTijdFileBegin']\n",
    "\n",
    "features = [\n",
    "    \"HectometerKop\",\n",
    "    \"SecondsSinceMidnight_Begin\",\n",
    "    \"Diesel\",\n",
    "    \"Aantal_check_ins_previous\",\n",
    "    \"Aantal_check_ins_current\",\n",
    "    \"Gasoline\",\n",
    "    \"LPG\",\n",
    "    \"P\",\n",
    "    \"T\",\n",
    "    \"KopWegvakVan_encoded\",\n",
    "    \"KopWegvakNaar_encoded\",\n",
    "    \"U\",\n",
    "    \"Year\",\n",
    "    \"hectometreringsrichting\",\n",
    "    \"TrajVan_encoded\",\n",
    "    \"provincie_x_encoded\",\n",
    "    \"R\",\n",
    "    \"OorzaakCode\",\n",
    "    \"TrajNaar_encoded\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 28.00078410696901\n",
      "Test MSE: 2099.8670299269047\n",
      "Test R2: 0.13908361119398616\n"
     ]
    }
   ],
   "source": [
    "X = df[features]\n",
    "y = df['FileZwaarte']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline that scales the data then applies RandomForestRegressor\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        max_features='sqrt', \n",
    "        max_depth=20, \n",
    "        min_samples_split=10, \n",
    "        min_samples_leaf=4, \n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test R2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 28.00966866495624\n",
      "Test MSE: 2084.9169336446416\n",
      "Test R2: 0.1452129435375089\n"
     ]
    }
   ],
   "source": [
    "X = df[features]\n",
    "y = df['FileZwaarte']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline that scales the data then applies XGBRegressor\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=1.0,\n",
    "        colsample_bytree=1.0,\n",
    "        min_child_weight=9,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test R2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2517\n",
      "[LightGBM] [Info] Number of data points in the train set: 68761, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 31.443126\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Test MAE: 27.906966380919926\n",
      "Test MSE: 2073.2693248413198\n",
      "Test R2: 0.1499883017703253\n"
     ]
    }
   ],
   "source": [
    "X = df[features]\n",
    "y = df['FileZwaarte']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline that scales the data then applies LGBMRegressor\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        min_child_weight=1,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test R2: {test_r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
